# P6.1.8 Robots.txt Implementation Plan

## 1. Overview and Objectives

### 1.1 Purpose
This document provides detailed step-by-step instructions for implementing and configuring robots.txt for The Strengths Toolbox website. The robots.txt file tells search engine crawlers which pages they can and cannot access.

### 1.2 Scope
This implementation plan covers task P6.1.8:
- **P6.1.8**: Configure robots.txt
  - Create or update robots.txt file
  - Configure allowed/disallowed paths
  - Add sitemap reference
  - Handle different environments (development, staging, production)
  - Implement dynamic robots.txt (optional)

### 1.3 Success Criteria
- robots.txt file accessible at `/robots.txt`
- Admin area disallowed from crawling
- API endpoints disallowed
- Search results disallowed
- Sitemap location specified
- Proper format and syntax
- Works in all environments

## 2. Prerequisites

### 2.1 Required Knowledge
- Robots.txt protocol
- Search engine crawling behavior
- Laravel routing

### 2.2 Dependencies
- Task P6.1.7 completed (Sitemap created)
- Routes defined in `routes/web.php`
- Admin routes defined

### 2.3 Reference Documents
- SEO Architecture: `documentation/01-architecture/05-seo-architecture.md`
- Robots.txt Protocol: https://www.robotstxt.org/
- Google Robots.txt Guidelines: https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt

## 3. Existing Implementation Review

### 3.1 Current State
- Static `robots.txt` file exists in `public/robots.txt`
- File may need updates for current site structure

### 3.2 What Needs to Be Done
- Review and update robots.txt content
- Ensure all disallowed paths are correct
- Add sitemap reference
- Consider dynamic robots.txt for environment-specific configuration

## 4. Task P6.1.8: Configure Robots.txt

### 4.1 Overview
Configure robots.txt to guide search engine crawlers, disallow admin areas and unnecessary pages, and reference the sitemap.

### 4.2 Step-by-Step Implementation

#### Step 1: Review Current Robots.txt

**File: `public/robots.txt`**

**Action:** Read current file and note what needs updating.

#### Step 2: Update Static Robots.txt

**File: `public/robots.txt`**

**Update required:**
```
User-agent: *
Allow: /

# Disallow admin area
Disallow: /admin/
Disallow: /admin

# Disallow API endpoints
Disallow: /api/

# Disallow search results (to avoid duplicate content)
Disallow: /search?
Disallow: /blog/search?

# Disallow health check endpoints
Disallow: /health
Disallow: /health/

# Allow specific important paths
Allow: /blog/
Allow: /contact
Allow: /about-us
Allow: /strengths-programme

# Sitemap location
Sitemap: https://www.thestrengthstoolbox.com/sitemap.xml
```

**Note:** Replace `https://www.thestrengthstoolbox.com` with your actual domain.

#### Step 3: Create Dynamic Robots.txt Controller (Optional but Recommended)

**File: `app/Http/Controllers/RobotsController.php`**

**Purpose:** Generate robots.txt dynamically based on environment

```php
<?php

namespace App\Http\Controllers;

use Illuminate\Http\Response;

class RobotsController extends Controller
{
    /**
     * Generate robots.txt content
     *
     * @return Response
     */
    public function index(): Response
    {
        $content = $this->generateRobotsTxt();

        return response($content, 200)
            ->header('Content-Type', 'text/plain; charset=UTF-8');
    }

    /**
     * Generate robots.txt content based on environment
     *
     * @return string
     */
    protected function generateRobotsTxt(): string
    {
        $lines = [];

        // In production, allow all crawlers
        // In staging/development, disallow all crawlers
        if (app()->environment('production')) {
            $lines[] = 'User-agent: *';
            $lines[] = 'Allow: /';
            $lines[] = '';
            $lines[] = '# Disallow admin area';
            $lines[] = 'Disallow: /admin/';
            $lines[] = 'Disallow: /admin';
            $lines[] = '';
            $lines[] = '# Disallow API endpoints';
            $lines[] = 'Disallow: /api/';
            $lines[] = '';
            $lines[] = '# Disallow search results';
            $lines[] = 'Disallow: /search?';
            $lines[] = 'Disallow: /blog/search?';
            $lines[] = '';
            $lines[] = '# Disallow health check endpoints';
            $lines[] = 'Disallow: /health';
            $lines[] = 'Disallow: /health/';
            $lines[] = '';
            $lines[] = '# Allow important paths';
            $lines[] = 'Allow: /blog/';
            $lines[] = 'Allow: /contact';
            $lines[] = 'Allow: /about-us';
            $lines[] = 'Allow: /strengths-programme';
            $lines[] = '';
            $lines[] = '# Sitemap location';
            $lines[] = 'Sitemap: ' . config('app.url') . '/sitemap.xml';
        } else {
            // Disallow all crawlers in non-production environments
            $lines[] = 'User-agent: *';
            $lines[] = 'Disallow: /';
            $lines[] = '';
            $lines[] = '# This is a ' . app()->environment() . ' environment';
            $lines[] = '# Crawling is disabled';
        }

        return implode("\n", $lines);
    }
}
```

#### Step 4: Add Robots.txt Route

**File: `routes/web.php`**

**Update required:**
Add route for dynamic robots.txt (before catch-all routes):

```php
use App\Http\Controllers\RobotsController;

// Robots.txt (before catch-all routes)
Route::get('/robots.txt', [RobotsController::class, 'index'])->name('robots');
```

**Note:** If using dynamic robots.txt, you can remove or keep the static file. The route will take precedence.

#### Step 5: Update .htaccess (If Using Static File)

**File: `public/.htaccess`**

**Action:** Ensure robots.txt is accessible. Usually no changes needed, but verify:

```apache
# Ensure robots.txt is accessible
<Files "robots.txt">
    Require all granted
</Files>
```

#### Step 6: Create Artisan Command to Validate Robots.txt

**File: `app/Console/Commands/ValidateRobotsTxt.php`**

**Purpose:** Command to validate robots.txt syntax and content

```php
<?php

namespace App\Console\Commands;

use Illuminate\Console\Command;
use Illuminate\Support\Facades\Http;

class ValidateRobotsTxt extends Command
{
    protected $signature = 'seo:validate-robots-txt';
    protected $description = 'Validate robots.txt file';

    public function handle(): int
    {
        $this->info('Validating robots.txt...');
        $this->newLine();

        $url = config('app.url') . '/robots.txt';
        
        try {
            $response = Http::get($url);
            
            if (!$response->successful()) {
                $this->error("Failed to fetch robots.txt: HTTP {$response->status()}");
                return 1;
            }

            $content = $response->body();
            $this->info("✓ robots.txt is accessible");
            $this->newLine();
            $this->line("Content:");
            $this->line($content);
            $this->newLine();

            // Basic validation
            $errors = [];
            
            // Check for sitemap
            if (!str_contains($content, 'Sitemap:')) {
                $errors[] = 'Missing Sitemap declaration';
            }

            // Check for User-agent
            if (!str_contains($content, 'User-agent:')) {
                $errors[] = 'Missing User-agent declaration';
            }

            // Check for common syntax errors
            if (preg_match('/Disallow:\s*$/', $content)) {
                $errors[] = 'Found empty Disallow directive (should be Disallow: /)';
            }

            if (empty($errors)) {
                $this->info('✓ robots.txt is valid!');
                return 0;
            }

            $this->error('Found issues:');
            foreach ($errors as $error) {
                $this->line("  ✗ {$error}");
            }

            return 1;
        } catch (\Exception $e) {
            $this->error("Error validating robots.txt: {$e->getMessage()}");
            return 1;
        }
    }
}
```

## 5. Testing Instructions

### 5.1 Manual Testing

1. **Test Robots.txt Access:**
   - Visit `/robots.txt`
   - Verify file is accessible
   - Verify Content-Type is `text/plain`

2. **Test Content:**
   - Verify User-agent directives are present
   - Verify Disallow directives are correct
   - Verify Sitemap location is present and correct

3. **Test in Different Environments:**
   - Test in development (should disallow all)
   - Test in staging (should disallow all)
   - Test in production (should allow with restrictions)

### 5.2 Automated Testing

1. **Run validation command:**
   ```bash
   php artisan seo:validate-robots-txt
   ```

2. **Use Online Validators:**
   - Google Search Console: Test robots.txt
   - https://www.google.com/webmasters/tools/robots-testing-tool

3. **Test with curl:**
   ```bash
   curl -I https://www.thestrengthstoolbox.com/robots.txt
   ```

### 5.3 Search Engine Testing

1. **Google Search Console:**
   - Submit robots.txt to Google Search Console
   - Verify no errors

2. **Test Crawling:**
   - Use Google Search Console URL Inspection
   - Verify blocked URLs are actually blocked
   - Verify allowed URLs are accessible

## 6. Success Criteria Validation

- [ ] robots.txt accessible at `/robots.txt`
- [ ] Admin area disallowed
- [ ] API endpoints disallowed
- [ ] Search results disallowed
- [ ] Sitemap location specified
- [ ] Proper format and syntax
- [ ] Works in production environment
- [ ] Blocks crawlers in non-production environments
- [ ] No syntax errors

## 7. Notes and Considerations

1. **Environment-Specific Configuration:**
   - Production: Allow crawling with restrictions
   - Staging/Development: Disallow all crawling
   - Use dynamic robots.txt for flexibility

2. **Common Disallow Patterns:**
   - `/admin/` - Admin panel
   - `/api/` - API endpoints
   - `/search?` - Search results (to avoid duplicate content)
   - `/health` - Health check endpoints
   - `/tmp/` - Temporary files
   - `/cache/` - Cache directories

3. **Sitemap Location:**
   - Must be absolute URL
   - Should use HTTPS in production
   - Should match actual sitemap location

4. **User-Agent Specific Rules:**
   - Can add specific rules for different crawlers
   - Example: Different rules for Googlebot vs Bingbot

5. **Crawl Delay:**
   - Can add `Crawl-delay:` directive if needed
   - Not widely supported, use with caution

6. **Wildcards:**
   - `*` matches any sequence of characters
   - `$` matches end of URL
   - Example: `Disallow: /*?$` blocks URLs ending with `?`

7. **Future Enhancements:**
   - Add sitemap index reference if using multiple sitemaps
   - Add specific rules for different user agents
   - Monitor robots.txt access in logs

---

**Document Version:** 1.0  
**Date Created:** 2025-01-27  
**Status:** Ready for Implementation
